import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from streamlit_drawable_canvas import st_canvas
from PIL import Image
import mlflow
import os
import random
from datetime import datetime

# T·∫£i d·ªØ li·ªáu MNIST t·ª´ OpenML
@st.cache_data
def load_mnist_data():
    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)
    X = X / 255.0  # Chu·∫©n h√≥a ngay khi t·∫£i
    return X, y.astype(int)

# Tab hi·ªÉn th·ªã d·ªØ li·ªáu
def data_processing():
    st.header("üìò D·ªØ Li·ªáu MNIST")
    X, y = load_mnist_data()
    
    st.write("""
        **Th√¥ng tin t·∫≠p d·ªØ li·ªáu MNIST:**
        - T·ªïng s·ªë m·∫´u: {}
        - K√≠ch th∆∞·ªõc m·ªói ·∫£nh: 28 √ó 28 pixels (784 ƒë·∫∑c tr∆∞ng)
        - S·ªë l·ªõp: 10 (ch·ªØ s·ªë t·ª´ 0-9)
    """.format(X.shape[0]))

    st.subheader("M·ªôt s·ªë h√¨nh ·∫£nh m·∫´u")
    n_samples = 5
    fig, axes = plt.subplots(1, n_samples, figsize=(10, 3))
    indices = np.random.choice(X.shape[0], n_samples, replace=False)
    for i, idx in enumerate(indices):
        axes[i].imshow(X[idx].reshape(28, 28), cmap='gray')
        axes[i].set_title(f"Label: {y[idx]}")
        axes[i].axis("off")
    st.pyplot(fig)

# Tab chia d·ªØ li·ªáu
def split_data():
    st.title("üìå Chia d·ªØ li·ªáu Train/Test")
    X, y = load_mnist_data()
    total_samples = X.shape[0]

    if "data_split_done" not in st.session_state:
        st.session_state.data_split_done = False

    num_samples = st.slider("Ch·ªçn s·ªë l∆∞·ª£ng ·∫£nh ƒë·ªÉ train:", 1000, total_samples, 10000)
    test_size_percent = st.slider("Ch·ªçn t·ª∑ l·ªá test (%):", 10, 50, 20)  # ƒê·ªïi sang ph·∫ßn trƒÉm
    test_size = test_size_percent / 100  # Chuy·ªÉn ƒë·ªïi sang d·∫°ng th·∫≠p ph√¢n ƒë·ªÉ s·ª≠ d·ª•ng trong train_test_split

    if st.button("‚úÖ X√°c nh·∫≠n & L∆∞u") and not st.session_state.data_split_done:
        st.session_state.data_split_done = True
        X_selected, _, y_selected, _ = train_test_split(X, y, train_size=num_samples, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X_selected, y_selected, test_size=test_size, random_state=42)

        st.session_state.total_samples = num_samples
        st.session_state.X_train = X_train
        st.session_state.X_test = X_test
        st.session_state.y_train = y_train
        st.session_state.y_test = y_test
        st.session_state.train_size = X_train.shape[0]
        st.session_state.test_size = X_test.shape[0]

        summary_df = pd.DataFrame({
            "T·∫≠p d·ªØ li·ªáu": ["Train", "Test"],
            "S·ªë l∆∞·ª£ng m·∫´u": [X_train.shape[0], X_test.shape[0]]
        })
        st.success(f"üîπ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia: Train ({len(X_train)}), Test ({len(X_test)})")
        st.table(summary_df)

    elif st.session_state.data_split_done:
        st.info("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia, kh√¥ng c·∫ßn ch·∫°y l·∫°i.")
# Tab hu·∫•n luy·ªán v√† ph√¢n c·ª•m

import streamlit as st
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
import mlflow
import mlflow.sklearn
import os

import streamlit as st
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from scipy.stats import mode
import mlflow
import mlflow.sklearn
import os

def train_evaluate():
    st.header("‚öôÔ∏è Ch·ªçn m√¥ h√¨nh & Hu·∫•n luy·ªán")

    # Ki·ªÉm tra d·ªØ li·ªáu tr∆∞·ªõc khi train
    if "X_train" not in st.session_state:
        st.warning("‚ö†Ô∏è Vui l√≤ng chia d·ªØ li·ªáu tr∆∞·ªõc khi train!")
        return

    X_train = st.session_state["X_train"]
    y_train = st.session_state["y_train"]

    # üåü **Chu·∫©n h√≥a d·ªØ li·ªáu**
    X_train = X_train.reshape(-1, 28 * 28) / 255.0

    # üìå **Ch·ªçn m√¥ h√¨nh**
    model_choice = st.selectbox("Ch·ªçn m√¥ h√¨nh:", ["K-Means", "DBSCAN"])

    if model_choice == "K-Means":
        st.markdown("üîπ **K-Means**: Thu·∫≠t to√°n ph√¢n c·ª•m chia d·ªØ li·ªáu th√†nh K nh√≥m d·ª±a tr√™n kho·∫£ng c√°ch.")
        n_clusters = st.slider("üî¢ Ch·ªçn s·ªë c·ª•m (K):", 2, 20, 10)
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        params = {"n_clusters": n_clusters}
    elif model_choice == "DBSCAN":
        st.markdown("üõ†Ô∏è **DBSCAN**: Thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô.")
        eps = st.slider("üìè B√°n k√≠nh l√¢n c·∫≠n (eps):", 0.1, 10.0, 0.5)
        min_samples = st.slider("üë• S·ªë ƒëi·ªÉm t·ªëi thi·ªÉu trong c·ª•m:", 2, 20, 5)
        model = DBSCAN(eps=eps, min_samples=min_samples)
        params = {"eps": eps, "min_samples": min_samples}

    # Gi·∫£m chi·ªÅu d·ªØ li·ªáu b·∫±ng PCA tr∆∞·ªõc khi hu·∫•n luy·ªán
    pca = PCA(n_components=2)
    X_train_pca = pca.fit_transform(X_train)

    if st.button("üöÄ Hu·∫•n luy·ªán m√¥ h√¨nh"):
        st.write(f"‚è≥ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh '{model_choice}'...")
        with st.spinner("ƒêang x·ª≠ l√Ω d·ªØ li·ªáu v√† hu·∫•n luy·ªán..."):
            # üéØ **T√≠ch h·ª£p MLflow**
            with mlflow.start_run(run_name=f"Train_{st.session_state.get('run_name', 'default_run')}") as run:
                run_id = run.info.run_id
                
                # Log c√°c tham s·ªë
                mlflow.log_params({"model_type": model_choice, **params})
                mlflow.log_param("train_size", X_train.shape[0])
                if "total_samples" in st.session_state:
                    mlflow.log_param("num_samples", st.session_state.total_samples)
                mlflow.log_param("pca_components", 2)  # Log s·ªë chi·ªÅu PCA

                # L∆∞u d·ªØ li·ªáu t·∫°m th·ªùi v√† log artifact
                os.makedirs("mlflow_artifactsb5", exist_ok=True)
                dataset_path = "mlflow_artifactsb5/dataset.npz"
                np.savez(dataset_path, X_train=X_train, y_train=y_train, X_train_pca=X_train_pca)
                mlflow.log_artifact(dataset_path)

                # Hu·∫•n luy·ªán m√¥ h√¨nh
                model.fit(X_train_pca)
                labels = model.labels_ if model_choice == "K-Means" else model.fit_predict(X_train_pca)
                
                # ƒê√°nh gi√° tr√™n t·∫≠p train
                if model_choice == "K-Means":
                    label_mapping = {}
                    for i in range(n_clusters):
                        mask = labels == i
                        if np.sum(mask) > 0:
                            most_common_label = mode(y_train[mask], keepdims=True).mode[0]
                            label_mapping[i] = most_common_label
                    predicted_labels = np.array([label_mapping[label] for label in labels])
                    accuracy = accuracy_score(y_train, predicted_labels)
                    st.success(f"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!")
                    st.write(f"üéØ **ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh:** `{accuracy * 100:.2f}%`")
                    mlflow.log_metric("train_accuracy", accuracy)
                else:  # DBSCAN
                    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
                    st.success(f"‚úÖ Hu·∫•n luy·ªán th√†nh c√¥ng!")
                    st.write(f"üéØ **S·ªë c·ª•m t√¨m ƒë∆∞·ª£c:** `{n_clusters_}`")
                    mlflow.log_metric("n_clusters", n_clusters_)

                # Log m√¥ h√¨nh
                mlflow.sklearn.log_model(model, model_choice.lower())

            # üîç L∆∞u m√¥ h√¨nh v√†o session_state
            if "models" not in st.session_state:
                st.session_state["models"] = []

            model_name = model_choice.lower().replace(" ", "_")
            count = 1
            new_model_name = model_name
            while any(m["name"] == new_model_name for m in st.session_state["models"]):
                new_model_name = f"{model_name}_{count}"
                count += 1

            st.session_state["models"].append({"name": new_model_name, "model": model})
            st.write(f"üîπ **M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v·ªõi t√™n:** `{new_model_name}`")
            st.write(f"üìã **Danh s√°ch c√°c m√¥ h√¨nh:** {[m['name'] for m in st.session_state['models']]}")

            # Hi·ªÉn th·ªã link MLflow
            mlflow_tracking_uri = "https://dagshub.com/TonThatTruongVu/MNIST-ClusteringAlgorithms.mlflow"
            experiment_id = mlflow.get_experiment_by_name("Clustering").experiment_id
            mlflow_link = f"{mlflow_tracking_uri}/#/experiments/{experiment_id}/runs/{run_id}"
            st.success(f"‚úÖ ƒê√£ log d·ªØ li·ªáu cho **Train_{st.session_state.get('run_name', 'default_run')}**!")
            st.markdown(f"üîó [Truy c·∫≠p MLflow UI]({mlflow_link})")



from PIL import Image
import numpy as np

def preprocess_canvas_image(canvas_result):
    """X·ª≠ l√Ω h√¨nh ·∫£nh t·ª´ canvas th√†nh ƒë·ªãnh d·∫°ng ph√π h·ª£p v·ªõi MNIST (784 chi·ªÅu)."""
    if canvas_result.image_data is not None:
        try:
            # Chuy·ªÉn d·ªØ li·ªáu canvas th√†nh ·∫£nh PIL
            img = Image.fromarray(canvas_result.image_data.astype(np.uint8))
            # Chuy·ªÉn th√†nh grayscale
            img_gray = img.convert("L")
            # Resize v·ªÅ 28x28
            img_resized = img_gray.resize((28, 28), Image.Resampling.LANCZOS)
            # Chuy·ªÉn th√†nh m·∫£ng NumPy v√† chu·∫©n h√≥a v·ªÅ [0, 1]
            img_normalized = np.array(img_resized) / 255.0
            return img_normalized.reshape(1, -1)  # Reshape th√†nh (1, 784)
        except Exception as e:
            st.error(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω ·∫£nh t·ª´ canvas: {str(e)}")
            return None
    return None

from PIL import Image
import numpy as np

def preprocess_uploaded_image(uploaded_file):
    """X·ª≠ l√Ω ·∫£nh t·∫£i l√™n th√†nh ƒë·ªãnh d·∫°ng ph√π h·ª£p v·ªõi MNIST (784 chi·ªÅu)."""
    if uploaded_file is not None:
        try:
            # ƒê·ªçc ·∫£nh t·ª´ file t·∫£i l√™n
            img = Image.open(uploaded_file).convert("L")  # Chuy·ªÉn sang grayscale
            # Resize v·ªÅ 28x28
            img_resized = img.resize((28, 28), Image.Resampling.LANCZOS)
            # Chuy·ªÉn th√†nh m·∫£ng NumPy v√† chu·∫©n h√≥a v·ªÅ [0, 1]
            img_normalized = np.array(img_resized) / 255.0
            return img_normalized.reshape(1, -1)  # Reshape th√†nh (1, 784)
        except Exception as e:
            st.error(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω ·∫£nh t·∫£i l√™n: {str(e)}")
            return None
    return None
def demo():
    st.header("‚úçÔ∏è V·∫Ω s·ªë ho·∫∑c t·∫£i ·∫£nh ƒë·ªÉ d·ª± ƒëo√°n c·ª•m")

    # Ki·ªÉm tra xem c√≥ m√¥ h√¨nh n√†o ƒë√£ hu·∫•n luy·ªán ch∆∞a
    if "models" not in st.session_state or not st.session_state["models"]:
        st.error("‚ö†Ô∏è M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán! Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh trong tab 'Hu·∫•n luy·ªán & ƒê√°nh gi√°' tr∆∞·ªõc.")
        return

    # Dropdown ch·ªçn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán t·ª´ st.session_state["models"]
    st.subheader("üîç Ch·ªçn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán")
    model_names = [model["name"] for model in st.session_state["models"]]
    model_option = st.selectbox("Ch·ªçn m√¥ h√¨nh:", model_names)
    model = next(model["model"] for model in st.session_state["models"] if model["name"] == model_option)

    # Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu
    input_method = st.selectbox("üìå Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p:", ["V·∫Ω s·ªë", "T·∫£i ·∫£nh"])

    if input_method == "V·∫Ω s·ªë":
        st.subheader("V·∫Ω s·ªë")
        if "key_value" not in st.session_state:
            st.session_state.key_value = str(random.randint(0, 1000000))
        if st.button("üîÑ T·∫£i l·∫°i n·∫øu kh√¥ng th·∫•y canvas"):
            st.session_state.key_value = str(random.randint(0, 1000000))
        canvas_result = st_canvas(
            fill_color="black",
            stroke_width=10,
            stroke_color="white",
            background_color="black",
            height=300,
            width=300,
            drawing_mode="freedraw",
            key=st.session_state.key_value,
            update_streamlit=True
        )
        input_data = preprocess_canvas_image(canvas_result)
        source = "v√πng v·∫Ω"
    else:  # T·∫£i ·∫£nh
        st.subheader("T·∫£i ·∫£nh")
        uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh s·ªë (jpg, png)...", type=["jpg", "png"])
        input_data = preprocess_uploaded_image(uploaded_file)
        source = "·∫£nh t·∫£i l√™n"

    if st.button("D·ª± ƒëo√°n c·ª•m"):
        if input_data is not None:
            # Hi·ªÉn th·ªã ·∫£nh ƒë√£ x·ª≠ l√Ω
            st.image(
                Image.fromarray((input_data.reshape(28, 28) * 255).astype(np.uint8)),
                caption=f"·∫¢nh x·ª≠ l√Ω t·ª´ {source}",
                width=100
            )

            # D·ª± ƒëo√°n c·ª•m tr√™n d·ªØ li·ªáu g·ªëc 784 chi·ªÅu
            if isinstance(model, KMeans):
                cluster = model.predict(input_data)[0]
                st.subheader(f"üî¢ C·ª•m d·ª± ƒëo√°n: {cluster}")
            elif isinstance(model, DBSCAN):
                cluster = model.fit_predict(input_data)[0]
                st.subheader(f"üî¢ C·ª•m d·ª± ƒëo√°n: {cluster if cluster != -1 else 'Nhi·ªÖu (kh√¥ng thu·ªôc c·ª•m)'}")
            show_experiment_selector()

        else:
            st.error(f"‚ö†Ô∏è H√£y {'v·∫Ω m·ªôt s·ªë' if input_method == 'V·∫Ω s·ªë' else 't·∫£i ·∫£nh'} tr∆∞·ªõc khi d·ª± ƒëo√°n!")

def show_experiment_selector():
    st.title(" MLflow Experiments ")
    experiment_name = "Clustering"
    experiments = mlflow.search_experiments()
    selected_experiment = next((exp for exp in experiments if exp.name == experiment_name), None)

    if not selected_experiment:
        st.error(f"‚ùå Experiment '{experiment_name}' kh√¥ng t·ªìn t·∫°i!")
        return

    st.subheader(f"üìå Experiment: {experiment_name}")
    st.write(f"**Experiment ID:** {selected_experiment.experiment_id}")
    st.write(f"**Tr·∫°ng th√°i:** {'Active' if selected_experiment.lifecycle_stage == 'active' else 'Deleted'}")

    runs = mlflow.search_runs(experiment_ids=[selected_experiment.experiment_id])
    if runs.empty:
        st.warning("‚ö† Kh√¥ng c√≥ runs n√†o trong experiment n√†y.")
        return

    st.write("### üèÉ‚Äç‚ôÇÔ∏è C√°c Runs g·∫ßn ƒë√¢y:")
    run_info = [(run["run_id"], run["params.run_name"] if "params.run_name" in run else f"Run {run['run_id'][:8]}") for _, run in runs.iterrows()]
    run_name_to_id = {name: rid for rid, name in run_info}
    selected_run_name = st.selectbox("üîç Ch·ªçn m·ªôt run:", list(run_name_to_id.keys()))
    selected_run_id = run_name_to_id[selected_run_name]

    selected_run = mlflow.get_run(selected_run_id)
    if selected_run:
        st.subheader(f"üìå Th√¥ng tin Run: {selected_run_name}")
        st.write(f"**Run ID:** {selected_run_id}")
        st.write(f"**Tr·∫°ng th√°i:** {selected_run.info.status}")
        start_time = datetime.fromtimestamp(selected_run.info.start_time / 1000).strftime("%Y-%m-%d %H:%M:%S") if selected_run.info.start_time else "Kh√¥ng c√≥ th√¥ng tin"
        st.write(f"**Th·ªùi gian ch·∫°y:** {start_time}")

        if selected_run.data.params:
            st.write("### ‚öôÔ∏è Parameters:")
            st.json(selected_run.data.params)
        if selected_run.data.metrics:
            st.write("### üìä Metrics:")
            st.json(selected_run.data.metrics)

def ly_thuyet_K_means():
    st.header("üìå L√Ω thuy·∫øt K-Means")
    st.write("""
    - **K-Means** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m **kh√¥ng gi√°m s√°t** (unsupervised learning) nh·∫±m chia d·ªØ li·ªáu th√†nh **K c·ª•m** (clusters) d·ª±a tr√™n s·ª± t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu. Thu·∫≠t to√°n s·ª≠ d·ª•ng **kho·∫£ng c√°ch Euclidean** ƒë·ªÉ ƒëo l∆∞·ªùng s·ª± g·∫ßn g≈©i gi·ªØa c√°c ƒëi·ªÉm v√† t√¢m c·ª•m (centroids).
    """)

    st.subheader("üîç C√°ch ho·∫°t ƒë·ªông chi ti·∫øt")
    st.markdown("""
    Thu·∫≠t to√°n K-Means ho·∫°t ƒë·ªông qua c√°c b∆∞·ªõc l·∫∑p ƒëi l·∫∑p l·∫°i nh∆∞ sau:
    """)

    # S·ª≠ d·ª•ng expander ƒë·ªÉ hi·ªÉn th·ªã t·ª´ng b∆∞·ªõc chi ti·∫øt
    with st.expander("1. Kh·ªüi t·∫°o t√¢m c·ª•m (Initialization)"):
        st.markdown("""
        - Ch·ªçn ng·∫´u nhi√™n **K ƒëi·ªÉm** t·ª´ t·∫≠p d·ªØ li·ªáu l√†m **t√¢m c·ª•m ban ƒë·∫ßu** (centroids).  
        - **V√≠ d·ª•**: V·ªõi K = 3, ch·ªçn 3 ƒëi·ªÉm ng·∫´u nhi√™n t·ª´ t·∫≠p MNIST l√†m c√°c t√¢m c·ª•m kh·ªüi ƒë·∫ßu.
        """)

    with st.expander("2. G√°n nh√£n c·ª•m (Assignment Step)"):
        st.markdown("""
        - V·ªõi m·ªói ƒëi·ªÉm d·ªØ li·ªáu trong t·∫≠p, t√≠nh **kho·∫£ng c√°ch Euclidean** ƒë·∫øn t·∫•t c·∫£ c√°c t√¢m c·ª•m.  
        - G√°n ƒëi·ªÉm ƒë√≥ v√†o c·ª•m c√≥ t√¢m g·∫ßn nh·∫•t.  
        - **C√¥ng th·ª©c kho·∫£ng c√°ch Euclidean**:  
        """)
        st.latex(r"d(x, c) = \sqrt{\sum_{i=1}^{n} (x_i - c_i)^2}")
        st.markdown("""
        Trong ƒë√≥:  
        - \( x \): ƒêi·ªÉm d·ªØ li·ªáu.  
        - \( c \): T√¢m c·ª•m.  
        - \( n \): S·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu (v·ªõi MNIST l√† 784).
        """)

    with st.expander("3. C·∫≠p nh·∫≠t t√¢m c·ª•m (Update Step)"):
        st.markdown("""
        - Sau khi g√°n t·∫•t c·∫£ ƒëi·ªÉm v√†o c√°c c·ª•m, t√≠nh l·∫°i **t√¢m c·ª•m m·ªõi** b·∫±ng c√°ch l·∫•y **trung b√¨nh t·ªça ƒë·ªô** c·ªßa m·ªçi ƒëi·ªÉm trong c·ª•m ƒë√≥.  
        - **C√¥ng th·ª©c**:  
        """)
        st.latex(r"c_j = \frac{1}{N_j} \sum_{x \in C_j} x")
        st.markdown("""
        Trong ƒë√≥:  
        - \( c_j \): T√¢m c·ª•m th·ª© \( j \).  
        - \( N_j \): S·ªë ƒëi·ªÉm trong c·ª•m \( j \).  
        - \( C_j \): T·∫≠p h·ª£p c√°c ƒëi·ªÉm thu·ªôc c·ª•m \( j \).
        """)

    with st.expander("4. L·∫∑p l·∫°i (Iteration)"):
        st.markdown("""
        - Quay l·∫°i b∆∞·ªõc 2, l·∫∑p l·∫°i qu√° tr√¨nh g√°n nh√£n v√† c·∫≠p nh·∫≠t t√¢m c·ª•m cho ƒë·∫øn khi:  
          - C√°c t√¢m c·ª•m kh√¥ng c√≤n thay ƒë·ªïi ƒë√°ng k·ªÉ (h·ªôi t·ª•).  
          - Ho·∫∑c ƒë·∫°t s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa (max iterations).
        """)

    st.subheader("üí° V√≠ d·ª• v·ªõi MNIST")
    st.markdown("""
    - N·∫øu K = 10 (s·ªë ch·ªØ s·ªë t·ª´ 0-9), K-Means s·∫Ω c·ªë g·∫Øng nh√≥m c√°c ·∫£nh ch·ªØ s·ªë th√†nh 10 c·ª•m.  
    - Ban ƒë·∫ßu, ch·ªçn 10 ·∫£nh ng·∫´u nhi√™n l√†m t√¢m. Sau v√†i l·∫ßn l·∫∑p, c√°c t√¢m c·ª•m d·∫ßn ƒë·∫°i di·ªán cho c√°c nh√≥m ch·ªØ s·ªë (v√≠ d·ª•: c·ª•m 0 ch·ª©a h·∫ßu h·∫øt ·∫£nh s·ªë 0).
    """)


def ly_thuyet_DBSCAN():
    st.header("üìå L√Ω thuy·∫øt DBSCAN")
    st.write("""
    - **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m **kh√¥ng gi√°m s√°t** d·ª±a tr√™n **m·∫≠t ƒë·ªô** c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu. 
    - Kh√°c v·ªõi K-Means, DBSCAN kh√¥ng y√™u c·∫ßu x√°c ƒë·ªãnh tr∆∞·ªõc s·ªë c·ª•m, m√† t·ª± ƒë·ªông t√¨m c√°c c·ª•m d·ª±a tr√™n ph√¢n b·ªë d·ªØ li·ªáu v√† c√≥ kh·∫£ nƒÉng ph√°t hi·ªán **nhi·ªÖu** (noise).
    """)

    st.subheader("üîç C√°ch ho·∫°t ƒë·ªông chi ti·∫øt")
    st.markdown("""
    DBSCAN ph√¢n c·ª•m d·ª±a tr√™n hai tham s·ªë ch√≠nh:  
    - **eps**: B√°n k√≠nh l√¢n c·∫≠n (kho·∫£ng c√°ch t·ªëi ƒëa gi·ªØa hai ƒëi·ªÉm ƒë·ªÉ coi l√† "g·∫ßn nhau").  
    - **min_samples**: S·ªë ƒëi·ªÉm t·ªëi thi·ªÉu trong v√πng l√¢n c·∫≠n ƒë·ªÉ h√¨nh th√†nh m·ªôt c·ª•m.  
    C√°c b∆∞·ªõc c·ª• th·ªÉ:
    """)

    # S·ª≠ d·ª•ng expander ƒë·ªÉ hi·ªÉn th·ªã t·ª´ng b∆∞·ªõc
    with st.expander("1. X√°c ƒë·ªãnh c√°c lo·∫°i ƒëi·ªÉm (Point Classification)"):
        st.markdown("""
        - **Core Point (ƒêi·ªÉm l√µi)**: M·ªôt ƒëi·ªÉm c√≥ √≠t nh·∫•t **min_samples** l√°ng gi·ªÅng (bao g·ªìm ch√≠nh n√≥) trong b√°n k√≠nh **eps**.  
        - **Border Point (ƒêi·ªÉm ranh gi·ªõi)**: Kh√¥ng ph·∫£i ƒëi·ªÉm l√µi, nh∆∞ng n·∫±m trong b√°n k√≠nh **eps** c·ªßa √≠t nh·∫•t m·ªôt ƒëi·ªÉm l√µi.  
        - **Noise Point (ƒêi·ªÉm nhi·ªÖu)**: Kh√¥ng ph·∫£i ƒëi·ªÉm l√µi, kh√¥ng n·∫±m trong b√°n k√≠nh **eps** c·ªßa b·∫•t k·ª≥ ƒëi·ªÉm l√µi n√†o.  
        - **V√≠ d·ª•**: V·ªõi MNIST, m·ªôt ƒëi·ªÉm l√µi c√≥ th·ªÉ l√† trung t√¢m c·ªßa v√πng ch·ªØ s·ªë "0", c√°c ƒëi·ªÉm ranh gi·ªõi l√† vi·ªÅn, v√† nhi·ªÖu l√† c√°c n√©t l·ªói.
        """)

    with st.expander("2. Kh·ªüi t·∫°o c·ª•m (Cluster Initialization)"):
        st.markdown("""
        - Ch·ªçn m·ªôt **ƒëi·ªÉm l√µi ch∆∞a thƒÉm** (unvisited core point) l√†m h·∫°t gi·ªëng (seed).  
        - T·∫°o c·ª•m m·ªõi t·ª´ ƒëi·ªÉm n√†y ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh ph√¢n c·ª•m.
        """)

    with st.expander("3. M·ªü r·ªông c·ª•m (Cluster Expansion)"):
        st.markdown("""
        - Th√™m t·∫•t c·∫£ c√°c ƒëi·ªÉm trong b√°n k√≠nh **eps** c·ªßa ƒëi·ªÉm l√µi v√†o c·ª•m.  
        - N·∫øu m·ªôt ƒëi·ªÉm ƒë∆∞·ª£c th√™m l√† ƒëi·ªÉm l√µi, ti·∫øp t·ª•c m·ªü r·ªông c·ª•m t·ª´ ƒëi·ªÉm ƒë√≥ (ƒë·ªá quy).  
        - **C√¥ng th·ª©c kho·∫£ng c√°ch Euclidean**:  
        """)
        st.latex(r"d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}")
        st.markdown("""
        Trong ƒë√≥:  
        - \( x, y \): Hai ƒëi·ªÉm d·ªØ li·ªáu.  
        - \( n \): S·ªë chi·ªÅu (784 v·ªõi MNIST).
        """)

    with st.expander("4. ƒê√°nh d·∫•u nhi·ªÖu v√† l·∫∑p l·∫°i"):
        st.markdown("""
        - C√°c ƒëi·ªÉm kh√¥ng thu·ªôc b·∫•t k·ª≥ c·ª•m n√†o ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† **nhi·ªÖu**.  
        - Ch·ªçn ƒëi·ªÉm l√µi ch∆∞a thƒÉm ti·∫øp theo, l·∫∑p l·∫°i qu√° tr√¨nh cho ƒë·∫øn khi t·∫•t c·∫£ ƒëi·ªÉm ƒë∆∞·ª£c x·ª≠ l√Ω.
        """)

    st.subheader("üí° V√≠ d·ª• v·ªõi MNIST")
    st.markdown("""
    - N·∫øu **eps = 0.5** v√† **min_samples = 5**, DBSCAN c√≥ th·ªÉ:  
      - T√¨m c√°c c·ª•m d√†y ƒë·∫∑c (nh∆∞ v√πng ch·ªØ s·ªë gi·ªëng nhau, v√≠ d·ª•: c√°c ·∫£nh "1" th·∫≥ng ƒë·ª©ng).  
      - Lo·∫°i b·ªè c√°c n√©t v·∫Ω b·∫•t th∆∞·ªùng ho·∫∑c c√°c ·∫£nh kh√°c bi·ªát l·ªõn (nh∆∞ "1" nghi√™ng qu√° xa) l√†m nhi·ªÖu.  
    - K·∫øt qu·∫£: S·ªë c·ª•m kh√¥ng c·ªë ƒë·ªãnh, ph·ª• thu·ªôc v√†o m·∫≠t ƒë·ªô d·ªØ li·ªáu.
    """)

def main():
    st.title("üñäÔ∏è MNIST Clustering with Streamlit & MLflow")
    
    if "mlflow_initialized" not in st.session_state:
        mlflow.set_tracking_uri("https://dagshub.com/TonThatTruongVu/MNIST-ClusteringAlgorithms.mlflow")
        os.environ["MLFLOW_TRACKING_USERNAME"] = "TonThatTruongVu"
        os.environ["MLFLOW_TRACKING_PASSWORD"] = "519c4a864e131de52197f54d170c130beb15ffd5"
        mlflow.set_experiment("Clustering")
        st.session_state.mlflow_url = "https://dagshub.com/TonThatTruongVu/MNIST-ClusteringAlgorithms.mlflow"
        st.session_state.mlflow_initialized = True

    tabs = st.tabs(["L√Ω thuy·∫øt K-Means", "L√Ω thuy·∫øt DBSCAN", "Data", "Hu·∫•n luy·ªán", "D·ª± ƒëo√°n"])
    
    with tabs[0]:
        ly_thuyet_K_means()
    with tabs[1]:
        ly_thuyet_DBSCAN()
    with tabs[2]:
        data_processing()
    with tabs[3]:
        split_data()
        train_evaluate()
    with tabs[4]:
        demo()

if __name__ == "__main__":
    main()